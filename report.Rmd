---
title: "Heuristic Approaches for the Prediction of Business Strengths"
author: "Stephen Dimig"
date: "November 12, 2015"
output: word_document
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(jpeg)
library(grid)
library(grid)
library(gridExtra)
library(jsonlite)
library(tm)
library(openNLP)
library(caret)

training <- read.csv("training.csv", stringsAsFactors=FALSE)
combined <- read.csv("combined.csv", stringsAsFactors=FALSE)
```

## Introduction
This report examines heuristic approaches to sample review data from the Yelp Dataset Challenge data and efficiently predict the strengths of a business. The tm package in R provides a framework that allows text mining and natural language processing techniques to be applied to datasets in a convenient way. We will leverage the tm package to analyze review data from the Yelp Dataset Challenge data set and determine the part of speech of each word in a review. The most frequently used nouns associated with positive reviews of a business are defined as strengths. 

### Glossary of Terms
| __Term__  | __Definition__  |
| -------------- | ----- |
| tm   | A text mining package for R with NLP support |
| NLP   | Natural Language Processing |
| Favorable Review   | A review with a rating of 3.5 or above |
| Similarity Score   | Percentage of words in the sampled data that are also in the full data  |
| Stopword   | A common word that is filtered out before processing review text |
| Strength   | A high frequency noun in the favorable reviews of a business |
| Popular   | A user that reviews a business that has many friends |
| Connected   | A user with many friends that reviewed the same business |
| Maven   | A user with many friends that reviewed the same business AND whose friends gave it the exact same rating |
| Boosting   | Predicting which technique is best for a given business and using it to improve the overall score |
| Random   | Randomly selected reviews about a business used as a baseline |

So, why not just crunch all of the data (or a high percentage of it) to determine the strengths? As wonderful as the tm package is, it is very slow. Examining a large number of reviews can take a very long time. A maximum sample size of up to 500 reviews was used in this analysis to determine the strengths and processing took up to an hour (`r summaryRprof("good_full.prof")$sampling.time` s). The goal here is to determine a technique that can make a reasonably accurate prediction using a smaller number of samples which could take just minutes (`r summaryRprof("good_sample.prof")$sampling.time` s).

## Methods and Data
### Heuristic Sampling Techniques 
A **Maven** is a trusted expert in a particular field, who seeks to pass knowledge on to others. I was introduced to the concept of a maven through Malcolm Gladwell's book "The Tipping Point". To find the mavens in this data we examined the friends of a user that reviewed a business to see if they gave the business the exact same rating. The potential maven user got a point for each friend that gave the same rating. The mavens were then sorted and only the top maven reviews were used for sampling.

A **Connected** user is similar to a maven, but they are scored only on the number of friends that also reviewed the same business with no weight given to the rating. The connected users were sorted by number of connections and only the top reviews were used for sampling.

A **Popular** user is one who reviewed a business that has a large number of friends. The friends may or may not have also reviewed the business. The popular users were sorted by the total number of friends and the most reviews were used for sampling.

**Boosting** attempts to predict the heuristic that will yield the best results (using machine learning techniques) and apply that one to the business.

For control purposes, a **Random** sample of reviews of the business taken was also used as a baseline.

### Use of tm Package for Part of Speech Tagging
Strengths are extracted from the review text in the following manner:

* All of the review texts are concatenated and collapsed into a single text
* All characters are converted to lower case
* Common English stopwords are removed
* The category of the business is also removed to keep it from showing up as a strength or weakness (ie; reviews of a Taco joint will mention Taco a lot)
* Each word is tagged with a part of speech
* All non-nouns are removed
* The word list is then sorted in descending order by frequency

The result looks like this graphically.

```{r, echo=FALSE,fig.height=2.5, fig.width=4, message=FALSE, warning=FALSE, output=FALSE}
number_raw_reviews <- 10

business <- stream_in(file("my_business.json"), verbose=FALSE)
reviews <- stream_in(file("my_reviews.json"), verbose=FALSE)

extractPOS <- function(x, thisPOSregex) {
    x <- as.String(x)
    wordAnnotation <- annotate(x, list(Maxent_Sent_Token_Annotator(), Maxent_Word_Token_Annotator()))
    POSAnnotation <- annotate(x, Maxent_POS_Tag_Annotator(), wordAnnotation)
    POSwords <- subset(POSAnnotation, type == "word")
    tags <- sapply(POSwords$features, '[[', "POS")
    thisPOSindex <- grep(thisPOSregex, tags)
    tokenizedAndTagged <- sprintf("%s/%s", x[POSwords][thisPOSindex], tags[thisPOSindex])
    untokenizedAndTagged <- paste(tokenizedAndTagged, collapse = " ")
    untokenizedAndTagged
}
good <- function(target_reviews, target_business) {
    my_reviews <- target_reviews[target_reviews$stars > 3, ]
    ssize <- ifelse(dim(my_reviews)[1]>=number_raw_reviews, number_raw_reviews, dim(my_reviews)[1])
    review_sample <- my_reviews[sample(nrow(my_reviews), ssize), ]
    review_text <- paste(review_sample$text, collapse=" ")
    nouns_review_text <- gsub("/NN", "", extractPOS(review_text, ".*NN$"))
    review_source <- VectorSource(nouns_review_text)
    corpus <- Corpus(review_source)
    corpus <- tm_map(corpus, content_transformer(tolower))
    corpus <- tm_map(corpus, removePunctuation)
    my_stopwords <- unlist(strsplit(tolower(paste(unlist(lapply(target_business$categories,  function (x) gsub("(.*)s$", "\\1", x))), collapse=" ")), " "))
    
    corpus <- tm_map(corpus, removeWords, c(stopwords("english"), "place", "spot", "way", my_stopwords))
    dtm <- DocumentTermMatrix(corpus)
    dtm2 <- as.matrix(dtm)
    frequency <- colSums(dtm2)
    frequency <- sort(frequency, decreasing=TRUE)
    frequency
}

good <- good(reviews, business)

wf <- data.frame(word=names(good), freq=good)   
p <- ggplot(subset(wf, freq>1), aes(word, freq, fill=word))    
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))  
p
```

### Basic Processing
The following procedure was used to test how well a sampling method performed. Random sampling is the control group. 

```{r fig.width=2.5, fig.height=2.5,echo=FALSE}

img <- readJPEG("basic_processing.jpg")
 grid.raster(img)
```

### Boosting
Boosting is performed due to a strange anamoly in the data where certain sampling techniques work well on subsets of data but perform only as well or below random sampling on the data as a whole.

```{r, echo=FALSE,fig.height=6, fig.width=6}
df <- read.csv("combined.csv", stringsAsFactors=FALSE)

types <- c("Maven", "Conn", "Pop", "Random")
values <- c(mean(training$good.maven[training$rating >= 3.5]), mean(training$good.conn[training$rating >= 3.5]), mean(training$good.pop[training$rating >= 3.5]), mean(training$good.random[training$rating >= 3.5]))
mydf <- data.frame(types, values)
names(mydf) <- c("Type", "Value")

p1 <- ggplot(mydf, aes(x=factor(Type), y=Value, fill=Type)) +
    geom_bar(stat = "identity") +
    geom_hline(aes(yintercept=mean(training$good.maven[training$rating >= 3.5]), colour="purple")) +
    ylab("Similarity") +
    xlab("") +
    theme(axis.text.x = element_text(angle = -90, vjust = 0.5)) + 
    ggtitle("Rating >= 3.5")

types <- c("Maven", "Conn", "Pop", "Random")
values <- c(mean(training$good.maven[training$rating >= 3.0 & training$rating < 3.5]), mean(training$good.conn[training$rating >= 3.0 & training$rating < 3.5]), mean(training$good.pop[training$rating >= 3.0 & training$rating < 3.5]), mean(training$good.random[training$rating >= 3.0 & training$rating < 3.5]))
mydf <- data.frame(types, values)
names(mydf) <- c("Type", "Value")

p2 <- ggplot(mydf, aes(x=factor(Type), y=Value, fill=Type)) +
    geom_bar(stat = "identity") +
    geom_hline(aes(yintercept=mean(training$good.conn[training$rating >= 3.0 & training$rating < 3.5]), colour="purple")) +
    ylab("Similarity") +
    xlab("") +
    theme(axis.text.x = element_text(angle = -90, vjust = 0.5)) + 
    ggtitle("3.0 <= Rating < 3.5")

types <- c("Maven", "Conn", "Pop", "Random")
values <- c(mean(training$good.maven[training$rating >= 0.0 & training$rating < 3.0]), mean(training$good.conn[training$rating >= 0.0 & training$rating < 3.0]), mean(training$good.pop[training$rating >= 0.0 & training$rating < 3.0]), mean(training$good.random[training$rating >= 0.0 & training$rating < 3.0]))
mydf <- data.frame(types, values)
names(mydf) <- c("Type", "Value")

p3 <- ggplot(mydf, aes(x=factor(Type), y=Value, fill=Type)) +
    geom_bar(stat = "identity") +
    geom_hline(aes(yintercept=mean(training$good.random[training$rating >= 0.0 & training$rating < 3.0]), colour="purple")) +
    ylab("Similarity") +
    xlab("") +
    theme(axis.text.x = element_text(angle = -90, vjust = 0.5)) + 
    ggtitle("0 <= Rating < 3.0")

grid.arrange(p1, p2, p3, ncol=2)
```


As you can see the maven technique performs much better than random when the overall when the rating is over 3.5, but not any better overall, and worse when the start rating is less than 3.0. The following is an example of how the modeling was done using the RandomForest method.


```{r}
create_models <- function() {    
    # ...
    good.mavenFit <- train(good.maven ~ rating + review.count + 
                                        good.review.count + 
                                        good.maven.count + 
                                        good.conn.count,
                           data=training,method="rf",
                           trc,
                           prox=TRUE,allowParallel=TRUE)
    
    # ...
}
```

## Results
Disappointingly, the results show that no sampling techniques perform significantly better than random sampling over a representative mix of businesses. This is in spite of the fact that both the Maven and Connectected techniques perform significantly better on businesses that were rated at 3.5 or above and boosting was used to pick a good heuristic for the business.

| __Technique__  | __Mean__  |
| -------------- | ----- |
| Boosting   | `r mean(combined$good.boost)` |
| Maven    | `r mean(combined$good.maven)` |
| Conn    | `r mean(combined$good.conn)` |
| Pop   | `r mean(combined$good.pop)` |
| Random   | `r mean(combined$good.random)` |


```{r, echo=FALSE,fig.height=3, fig.width=6}
df <- read.csv("combined.csv", stringsAsFactors=FALSE)

p1 <- ggplot(df, aes(name, x=id)) + 
    scale_colour_manual("", breaks=c("Boost", "Maven", "Conn", "Pop", "Random"), values=c('blue', 'orange', 'green', 'red', 'yellow')) +
    geom_line(aes(y = good.boost, colour="Boost")) + 
    geom_line(aes(y = good.maven, colour="Maven")) +
    geom_line(aes(y = good.conn, colour="Conn")) +
    geom_line(aes(y = good.pop, colour="Pop")) +
    geom_line(aes(y = good.random, colour="Random")) +
    ylab("Similarity") +
    xlab("") +
    ggtitle("Strengths")

types <- c("Boost", "Maven", "Conn", "Pop", "Random")
values <- c(mean(df$good.boost), mean(df$good.maven), mean(df$good.conn), mean(df$good.pop), mean(df$good.random))
mydf <- data.frame(types, values)
names(mydf) <- c("Type", "Value")

p2 <- ggplot(mydf, aes(x=factor(Type), y=Value, fill=Type)) +
    geom_bar(stat = "identity") +
    geom_hline(aes(yintercept=mean(df$good.boost), colour="purple")) +
    ylab("Similarity") +
    xlab("") +
    theme(axis.text.x = element_text(angle = -90, vjust = 0.5)) + 
    ggtitle("Strengths")


grid.arrange(p1, p2, ncol=2)
```

## Discussion
If I had to do this project over again, I would batch process the full data set with something like Hadoop to determine the top strengths and weaknesses of each business. R did not perform well performance wise on the computationally expensive data mining tasks. I had to run a task almost two full days just to get the results for 50 businesses. This left less time to explore and tune the different sampling techniques. Boosting could also be tuned with more runs and additional features so that it could better predict which heuristic to use. Even though the results were not what I expected, the challenge of working on the data set was fun and I can definitely use the experience on future projects. 
